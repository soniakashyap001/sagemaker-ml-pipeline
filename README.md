# ğŸ§  End-to-End ML Pipeline with Amazon SageMaker (XGBoost)

This project demonstrates how to build a complete machine learning pipeline on AWS SageMaker using the built-in XGBoost algorithm. The pipeline includes data upload, training, model deployment, and inference â€” all done programmatically using Boto3 and SageMaker SDK.

## ğŸš€ Steps Covered

- Creating an S3 bucket
- Uploading train/test data using Boto3
- Setting up SageMaker notebook instance with IAM roles
- Training a model using SageMaker's built-in XGBoost container
- Deploying the model for inference
- Making predictions and decoding outputs

## ğŸ“ Project Structure

```
sagemaker-ml-pipeline/
â”œâ”€â”€ data/           # Sample train/test CSVs (optional)
â”œâ”€â”€ notebook/       # Jupyter notebook with full code
â”œâ”€â”€ screenshots/    # Screenshots from AWS console
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“¸ Screenshots

| Creating S3 Bucket | Notebook Instance | Model Output |
| ------------------ | ----------------- | ------------ |
|                    |                   |              |

## ğŸ”— Medium Article

ğŸ“– [Building an End-to-End ML Pipeline with Amazon SageMaker (XGBoost)](https://medium.com/@soniakashyap001/building-an-end-to-end-ml-pipeline-with-amazon-sagemaker-xgboost-2fb580f6ce45)

## ğŸ›  Requirements

```bash
boto3
sagemaker
pandas
numpy
```

Install them with:

```bash
pip install -r requirements.txt
```

## ğŸ’¬ Author

**Sonia Kashyap**\
[LinkedIn](https://linkedin.com/in/soniakashyap001) â€¢ [Medium](https://medium.com/@soniakashyap001)

---

Feel free to fork this repo, open issues, or suggest improvements!

